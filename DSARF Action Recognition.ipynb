{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f504a26",
   "metadata": {},
   "source": [
    "# Action DSARF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsarf import DSARF, compute_NRMSE, ELBO_Loss\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.signal\n",
    "torch.manual_seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f0ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self, d_in, d_h, A):\n",
    "        super(classifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(d_in, d_h, 2, batch_first=True, bidirectional=True) \n",
    "        self.fc1 = nn.Linear(2*d_h, d_h)\n",
    "        self.fc2 = nn.Linear(d_h, A)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        # x: N x T x D\n",
    "        x, _ = self.rnn(x) # N x T x h\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class action_dsarf(nn.Module):\n",
    "    def __init__(self, D, factor_dim, L, S, A, rc = True,\n",
    "                 VI = {'rnn_dim': None, 'combine': False, 'S': False},\n",
    "                 fc = False, bs=100, lr = 1e-2, S2A=True):\n",
    "        super().__init__()\n",
    "        self.dsarf = nn.ModuleList([DSARF(D=d, factor_dim=k, L=l, S=s, VI=VI, factorization=fc, recurrent = rc)\n",
    "                                    for d, k, l, s in zip(D, factor_dim, L, S)])\n",
    "        self.cls = classifier([sum(S) if S2A else sum(factor_dim)][0], VI['rnn_dim'], A)\n",
    "        self.bs, self.lr, self.L, self.VI, self.grad = bs, lr, L, VI, True\n",
    "        self.S2A, self.A = S2A, A\n",
    "        \n",
    "    def fit(self, data, labels, epoch=500):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print('Using device:', device)\n",
    "        if self.grad:\n",
    "            for m, d in zip(self.dsarf, data):\n",
    "                data_cat = np.concatenate(d, axis = 0)\n",
    "                #m.mean = data_cat[~np.isnan(data_cat)].mean() #comment for unnormalized\n",
    "                #m.std = data_cat[~np.isnan(data_cat)].std() #comment\n",
    "        for p in self.parameters(): #turn gradients on/off\n",
    "            p.requires_grad  = self.grad\n",
    "        \n",
    "        data_in = data.copy()\n",
    "        #data = [[(di - m.mean)/m.std for di in d] for d, m in zip(data, self.dsarf)]\n",
    "        data = [np.array([(di - m.mean)/m.std for di in d]) for d, m in zip(data, self.dsarf)]\n",
    "        n_data = [len(d) for d in data]\n",
    "        lens = [[len(di) for di in d] for d in data]\n",
    "        \n",
    "        idxs = [torch.LongTensor([i]) for i in range(len(data[0]))]\n",
    "        \n",
    "        model = self.DSARF(self, n_data, lens).to(device)\n",
    "        optim_dsarf = optim.Adam(model.parameters(), lr = self.lr)\n",
    "        CELoss = nn.CrossEntropyLoss(reduction = 'sum')\n",
    "        \n",
    "        params = {'batch_size': self.bs,\n",
    "                  'shuffle': True,\n",
    "                  'num_workers': 0}\n",
    "        \n",
    "        train_loader = DataLoader(idxs, **params)\n",
    "        \n",
    "        for i in tqdm(range(epoch)):\n",
    "            loss_value = 0.0\n",
    "            acc = np.zeros((self.A, self.A))\n",
    "            for bidxs in train_loader:\n",
    "                bidxs = bidxs.reshape(-1)\n",
    "                #mb = [torch.FloatTensor([d[bidx] for bidx in bidxs]).to(device) for d in data]\n",
    "                mb = [torch.FloatTensor(d[bidxs]).to(device) for d in data]\n",
    "                outs = model.forward(mb, [bidxs]*len(data))\n",
    "                mask = [~torch.isnan(d) for d in mb]\n",
    "                loss_dsarf = [ELBO_Loss(d[m], out[0][m], out[1], out[2],\n",
    "                                      out[3][:, max(l):], out[4][:, max(l):],\n",
    "                                      out[5], out[6], out[7], out[8],\n",
    "                                      out[9][:, max(l):], out[10][:, max(l):],\n",
    "                                      out[11][:,:, max(l):], out[12][:,:, max(l):],\n",
    "                                      0.001) for d, out, m, l in zip(mb, outs, mask, self.L)]\n",
    "                loss_dsarf = sum(loss_dsarf)\n",
    "                if self.S2A:\n",
    "                    q_in = torch.cat([out[13] for out in outs], dim=-1) #q_s_t_log\n",
    "                else:\n",
    "                    q_in = torch.cat([out[9] for out in outs], dim=-1) #q_z_mus\n",
    "                target = model.cls.forward(q_in)\n",
    "                    \n",
    "                #loss_dsarf = loss_dsarf + CELoss(target.reshape(-1, self.A),\n",
    "                #             torch.LongTensor([labels[bidx] for bidx in bidxs]).reshape(-1).to(device))\n",
    "                loss_dsarf = loss_dsarf + CELoss(target.reshape(-1, self.A),\n",
    "                             torch.LongTensor(labels[bidxs]).reshape(-1).to(device))\n",
    "                \n",
    "                optim_dsarf.zero_grad()\n",
    "                loss_dsarf.backward()\n",
    "                optim_dsarf.step()\n",
    "                loss_value += loss_dsarf.item()\n",
    "                \n",
    "                target = target.argmax(-1).type(torch.FloatTensor).detach().cpu().numpy()\n",
    "                target = torch.LongTensor(scipy.signal.medfilt(target,[1,5]))\n",
    "                #acc += confusion_matrix(np.array([labels[bidx][0] for bidx in bidxs]),\n",
    "                #                        target.mode(dim=-1)[0].numpy(),\n",
    "                #                        labels = np.arange(len(acc))) \n",
    "                acc += confusion_matrix(labels[bidxs, 0],\n",
    "                                        target.mode(dim=-1)[0].numpy(),\n",
    "                                        labels = np.arange(len(acc))) \n",
    "                \n",
    "            if (i % 50 == 0) or (i == epoch - 1):\n",
    "                NRMSE = [m.report_stats(d) for m, d in zip(model.dsarf, data_in)]\n",
    "                NRMSE = {'NRMSE_recv': sum([e['NRMSE_recv'] for e in NRMSE])/len(NRMSE),\n",
    "                         'NRMSE_pred': sum([e['NRMSE_pred'] for e in NRMSE])/len(NRMSE)}\n",
    "                epch = i + 1\n",
    "                \n",
    "            print('ELBO_Loss: %0.4f, Accuracy: %0.2f, Epoch %d: {NRMSE_recv : %0.2f, NRMSE_pred : %0.2f}'\n",
    "                  % (loss_value / len(train_loader.dataset),\n",
    "                     acc.trace()/len(train_loader.dataset)*100, \n",
    "                     epch, NRMSE['NRMSE_recv'], NRMSE['NRMSE_pred']),\n",
    "                  end=\"\\r\", flush=True)\n",
    "        return model\n",
    "        \n",
    "    def infer(self, data, labels, epoch = 1):\n",
    "        self.grad = False\n",
    "        model = self.fit(data, labels, epoch)\n",
    "        self.grad = True\n",
    "        return model\n",
    "        \n",
    "    class DSARF(nn.Module):\n",
    "        def __init__(self, dsarf, n_data, lens):\n",
    "            super().__init__()\n",
    "            self.dsarf = nn.ModuleList([m.DSARF_(m, n, l) for m,n,l in zip(dsarf.dsarf, n_data, lens)])\n",
    "            self.cls = dsarf.cls\n",
    "        def forward(self, mb, mbi):\n",
    "            return [m.forward(i, j) for i,j, m in zip(mb, mbi, self.dsarf)]\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/train_data.npy'\n",
    "data = np.load(path).transpose(4, 0, 2, 3, 1)[0]\n",
    "T = data.shape[1]\n",
    "D = data.shape[2]\n",
    "data = data.reshape(-1, T, D, 3)\n",
    "data_val = np.load(path[:-14]+'val_data.npy').transpose(4, 0, 2, 3, 1)[0]\n",
    "data_val = data_val.reshape(-1,T,D,3)\n",
    "\n",
    "import pickle\n",
    "labels = pickle.load(open(path[:-14]+'train_label.pkl', 'rb'))[1]\n",
    "labels = np.tile(np.array(labels).reshape(-1,1), (1, T))\n",
    "labels_val = pickle.load(open(path[:-14]+'val_label.pkl', 'rb'))[1]\n",
    "labels_val = np.tile(np.array(labels_val).reshape(-1,1), (1, T))\n",
    "\n",
    "groups = [np.arange(25)]#[[13,14,15], [17,18,19], [0,12,16,1,4,20,8,2,3],\n",
    "          #[5,6,7,21,22],[9,10,11,23,24]] #group of joints LF, RF, T, LH, RH\n",
    "data_train = [[d[:,g].reshape(-1,len(g)*3) for d in data] for g in groups]\n",
    "data_test = [[d[:,g].reshape(-1,len(g)*3) for d in data_val] for g in groups]\n",
    "\n",
    "D = [d[0].shape[-1] for d in data_train]\n",
    "L = [[1,2]]*len(groups)\n",
    "K = [15]#[3,3,3,3,3] #[5]*len(groups)\n",
    "S = [20]#[4]*len(groups)\n",
    "VI = {'rnn_dim': K[0], 'combine': False, 'S': True}\n",
    "dsarf = action_dsarf(D, K, L, S, A=labels.max()+1, VI=VI, bs=3000, S2A=True)\n",
    "model_train = dsarf.fit(data_train, labels, 300)\n",
    "model_test = dsarf.infer(data_test, labels_val, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1859d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
